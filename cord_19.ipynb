{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab9a08a",
   "metadata": {},
   "source": [
    "\n",
    "# CORD-19 Dataset Analysis: COVID-19 Research Publications\n",
    "\n",
    "## Overview\n",
    "This notebook analyzes the CORD-19 dataset metadata to understand patterns in COVID-19 research publications. The analysis covers:\n",
    "\n",
    "- **Data Loading & Exploration**: Understanding the dataset structure and quality\n",
    "- **Publication Trends**: Temporal patterns in research output  \n",
    "- **Journal Analysis**: Top publishing venues and distribution\n",
    "- **Text Mining**: Word frequency and content analysis\n",
    "- **Visualizations**: Interactive charts and statistical summaries\n",
    "\n",
    "## Dataset Information\n",
    "- **Source**: [CORD-19 Dataset](https://www.semanticscholar.org/cord19/download)\n",
    "- **Content**: Metadata for COVID-19 research papers\n",
    "- **Size**: ~500MB+ (metadata only)\n",
    "- **Columns**: Title, abstract, authors, journal, publication date, DOI, etc.\n",
    "\n",
    "## Requirements\n",
    "```bash\n",
    "pip install pandas numpy matplotlib seaborn plotly wordcloud\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bce5615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Libraries loaded successfully!\n",
      "📅 Analysis date: 2025-09-19 17:20\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Text analysis\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"📚 Libraries loaded successfully!\")\n",
    "print(f\"📅 Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26fc978",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Data Loading and Basic Exploration\n",
    "\n",
    "In this section, we:\n",
    "1. Load the CORD-19 metadata CSV file\n",
    "2. Examine the basic structure and dimensions\n",
    "3. Identify data types and column information\n",
    "4. Assess data quality and completeness\n",
    "\n",
    "**Expected Time**: 2-3 hours (including data download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05048ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cord19_data(file_path=\"metadata.csv\", sample_size=None):    \n",
    "    print(\"📥 Loading CORD-19 metadata...\")\n",
    "    print(f\"   File: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        if sample_size:\n",
    "            print(f\"   Sampling: {sample_size:,} random papers\")\n",
    "            # Read in chunks for memory efficiency\n",
    "            chunk_list = []\n",
    "            chunk_size = 10000\n",
    "            \n",
    "            for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "                chunk_list.append(chunk)\n",
    "                if len(pd.concat(chunk_list)) >= sample_size * 1.2:  # Get extra for sampling\n",
    "                    break\n",
    "            \n",
    "            df = pd.concat(chunk_list, ignore_index=True)\n",
    "            df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "        else:\n",
    "            print(\"   Loading full dataset...\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "        print(f\"✅ Dataset loaded successfully!\")\n",
    "        print(f\"📊 Final shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "        print(f\"💾 Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ File not found!\")\n",
    "        print(\"📥 Please download metadata.csv from: https://www.semanticscholar.org/cord19/download\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading data: {e}\")\n",
    "        return None\n",
    "    \n",
    "# df = load_cord19_data(\"metadata.csv\")  # Full dataset\n",
    "df = load_cord19_data(\"metadata.csv\", sample_size=10000)  # Sample for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd205e1",
   "metadata": {},
   "source": [
    "## Basic Dataset Exploration\n",
    "\n",
    "Let's examine the structure and content of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"🔍 DATASET OVERVIEW\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    print(f\"Size in memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"Data types: {df.dtypes.value_counts().to_dict()}\")\n",
    "    \n",
    "    print(\"\\n📋 COLUMN INFORMATION:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        dtype = str(df[col].dtype)\n",
    "        non_null = df[col].count()\n",
    "        null_count = df[col].isnull().sum()\n",
    "        null_pct = (null_count / len(df)) * 100\n",
    "        \n",
    "        # Sample values\n",
    "        sample_vals = df[col].dropna().head(2).values\n",
    "        sample_str = str(sample_vals).replace('\\n', ' ')[:50] + '...' if len(str(sample_vals)) > 50 else str(sample_vals)\n",
    "        \n",
    "        print(f\"{i:2d}. {col:<20} | {dtype:<12} | {non_null:>7,} non-null | {null_pct:5.1f}% missing | {sample_str}\")\n",
    "    \n",
    "    print(f\"\\n📄 FIRST 3 ROWS:\")\n",
    "    display(df.head(3))\n",
    "    \n",
    "    print(f\"\\n📊 BASIC STATISTICS:\")\n",
    "    display(df.describe(include='all').round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1794f921",
   "metadata": {},
   "source": [
    "## Missing Data Analysis\n",
    "\n",
    "Understanding data completeness is crucial for reliable analysis. Let's examine missing data patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fbf3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_data(df):\n",
    "    \"\"\"Comprehensive missing data analysis with visualization\"\"\"\n",
    "    \n",
    "    print(\"🔍 MISSING DATA ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Calculate missing data statistics\n",
    "    missing_data = df.isnull().sum().sort_values(ascending=False)\n",
    "    missing_pct = (missing_data / len(df)) * 100\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': missing_data.index,\n",
    "        'Missing_Count': missing_data.values,\n",
    "        'Missing_Percent': missing_pct.values\n",
    "    })\n",
    "    \n",
    "    # Show only columns with missing data\n",
    "    missing_with_nulls = missing_df[missing_df['Missing_Count'] > 0]\n",
    "    \n",
    "    if len(missing_with_nulls) > 0:\n",
    "        print(\"📊 Columns with missing data:\")\n",
    "        for _, row in missing_with_nulls.head(10).iterrows():\n",
    "            print(f\"   {row['Column']:<25}: {row['Missing_Count']:>7,} ({row['Missing_Percent']:5.1f}%)\")\n",
    "    else:\n",
    "        print(\"✅ No missing data found!\")\n",
    "    \n",
    "    # Visualize missing data\n",
    "    if len(missing_with_nulls) > 0:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Missing data bar chart\n",
    "        top_missing = missing_with_nulls.head(10)\n",
    "        axes[0].barh(top_missing['Column'], top_missing['Missing_Percent'], color='coral', alpha=0.7)\n",
    "        axes[0].set_xlabel('Missing Data Percentage (%)')\n",
    "        axes[0].set_title('Missing Data by Column (Top 10)')\n",
    "        axes[0].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Missing data heatmap for key columns\n",
    "        key_columns = ['title', 'abstract', 'authors', 'journal', 'publish_time', 'doi']\n",
    "        available_key_cols = [col for col in key_columns if col in df.columns]\n",
    "        \n",
    "        if available_key_cols and len(available_key_cols) > 1:\n",
    "            missing_matrix = df[available_key_cols].head(1000).isnull()  # Sample for visualization\n",
    "            sns.heatmap(missing_matrix, cbar=True, yticklabels=False, cmap='viridis', ax=axes[1])\n",
    "            axes[1].set_title('Missing Data Heatmap (Key Columns, Sample)')\n",
    "            axes[1].set_xlabel('Columns')\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'Insufficient columns\\nfor heatmap', \n",
    "                        ha='center', va='center', transform=axes[1].transAxes, fontsize=14)\n",
    "            axes[1].set_title('Missing Data Heatmap')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "if df is not None:\n",
    "    missing_analysis = analyze_missing_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d949f",
   "metadata": {},
   "source": [
    " ## Comprehensive data cleaning for CORD-19 metadata\n",
    "    \n",
    "### Cleaning steps:\n",
    "    1. Remove papers without titles (essential for analysis)\n",
    "    2. Parse and validate publication dates\n",
    "    3. Create derived features (word counts, year extraction)\n",
    "    4. Handle journal name standardization\n",
    "    5. Filter for reasonable publication years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6820a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cord19_data(df):    \n",
    "    print(\"🧹 DATA CLEANING PIPELINE\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    original_size = len(df_clean)\n",
    "    \n",
    "    print(f\"📊 Starting with: {original_size:,} papers\")\n",
    "    \n",
    "    # Step 1: Remove papers without titles\n",
    "    if 'title' in df_clean.columns:\n",
    "        before_title = len(df_clean)\n",
    "        df_clean = df_clean.dropna(subset=['title'])\n",
    "        df_clean = df_clean[df_clean['title'].str.strip() != '']\n",
    "        removed_title = before_title - len(df_clean)\n",
    "        print(f\"🗑️  Removed {removed_title:,} papers without titles\")\n",
    "    \n",
    "    # Step 2: Handle publication dates\n",
    "    if 'publish_time' in df_clean.columns:\n",
    "        print(\"📅 Processing publication dates...\")\n",
    "        \n",
    "        # Convert to datetime\n",
    "        df_clean['publish_time'] = pd.to_datetime(df_clean['publish_time'], errors='coerce')\n",
    "        \n",
    "        # Extract year\n",
    "        df_clean['publication_year'] = df_clean['publish_time'].dt.year\n",
    "        \n",
    "        # Filter for reasonable years (1990-2024)\n",
    "        before_year_filter = len(df_clean)\n",
    "        valid_years = (df_clean['publication_year'] >= 1990) & (df_clean['publication_year'] <= 2024)\n",
    "        df_clean = df_clean[valid_years | df_clean['publication_year'].isnull()]\n",
    "        removed_years = before_year_filter - len(df_clean)\n",
    "        print(f\"🗑️  Removed {removed_years:,} papers with invalid years\")\n",
    "        \n",
    "        # Show year distribution\n",
    "        if 'publication_year' in df_clean.columns:\n",
    "            year_range = f\"{df_clean['publication_year'].min():.0f} - {df_clean['publication_year'].max():.0f}\"\n",
    "            print(f\"📅 Publication year range: {year_range}\")\n",
    "    \n",
    "    # Step 3: Create derived features\n",
    "    print(\"🔧 Creating derived features...\")\n",
    "    \n",
    "    # Abstract word count\n",
    "    if 'abstract' in df_clean.columns:\n",
    "        df_clean['abstract_word_count'] = df_clean['abstract'].astype(str).apply(\n",
    "            lambda x: len(x.split()) if pd.notna(x) and x.lower() not in ['nan', 'none', ''] else 0\n",
    "        )\n",
    "        avg_abstract_len = df_clean['abstract_word_count'].mean()\n",
    "        print(f\"📝 Average abstract length: {avg_abstract_len:.0f} words\")\n",
    "    \n",
    "    # Title characteristics\n",
    "    if 'title' in df_clean.columns:\n",
    "        df_clean['title_length'] = df_clean['title'].astype(str).apply(len)\n",
    "        df_clean['title_word_count'] = df_clean['title'].astype(str).apply(lambda x: len(x.split()))\n",
    "        avg_title_len = df_clean['title_length'].mean()\n",
    "        print(f\"📝 Average title length: {avg_title_len:.0f} characters\")\n",
    "    \n",
    "    # Step 4: Clean journal names\n",
    "    if 'journal' in df_clean.columns:\n",
    "        # Fill missing journals\n",
    "        df_clean['journal'] = df_clean['journal'].fillna('Unknown Journal')\n",
    "        \n",
    "        # Basic journal name cleaning\n",
    "        df_clean['journal'] = df_clean['journal'].str.strip()\n",
    "        df_clean['journal'] = df_clean['journal'].str.title()\n",
    "        \n",
    "        unique_journals = df_clean['journal'].nunique()\n",
    "        print(f\"📰 Found {unique_journals:,} unique journals\")\n",
    "    \n",
    "    # Step 5: Handle author information\n",
    "    if 'authors' in df_clean.columns:\n",
    "        df_clean['has_authors'] = df_clean['authors'].notna()\n",
    "        df_clean['author_count'] = df_clean['authors'].astype(str).apply(\n",
    "            lambda x: len(x.split(';')) if pd.notna(x) and x.lower() not in ['nan', 'none', ''] else 0\n",
    "        )\n",
    "        papers_with_authors = df_clean['has_authors'].sum()\n",
    "        print(f\"👥 Papers with author info: {papers_with_authors:,} ({papers_with_authors/len(df_clean)*100:.1f}%)\")\n",
    "    \n",
    "    final_size = len(df_clean)\n",
    "    removed_total = original_size - final_size\n",
    "    \n",
    "    print(f\"\\n✅ Cleaning completed!\")\n",
    "    print(f\"📊 Final dataset: {final_size:,} papers ({removed_total:,} removed, {removed_total/original_size*100:.1f}%)\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply cleaning\n",
    "if df is not None:\n",
    "    df_clean = clean_cord19_data(df)\n",
    "    print(f\"\\n🎯 Ready for analysis with {len(df_clean):,} papers!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c2dbf",
   "metadata": {},
   "source": [
    "## Data Quality Assessment\n",
    "\n",
    "Let's verify the quality of our cleaned dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbff30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df):    \n",
    "    print(\"🏆 DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_papers = len(df)\n",
    "    print(f\"📄 Total papers: {total_papers:,}\")\n",
    "    \n",
    "    # Completeness scores for key fields\n",
    "    key_fields = ['title', 'abstract', 'authors', 'journal', 'publication_year']\n",
    "    \n",
    "    print(f\"\\n📊 Completeness Assessment:\")\n",
    "    for field in key_fields:\n",
    "        if field in df.columns:\n",
    "            complete_count = df[field].notna().sum()\n",
    "            completeness = (complete_count / total_papers) * 100\n",
    "            status = \"✅\" if completeness > 90 else \"⚠️\" if completeness > 70 else \"❌\"\n",
    "            print(f\"   {status} {field:<18}: {completeness:5.1f}% complete ({complete_count:,}/{total_papers:,})\")\n",
    "    \n",
    "    # Data distribution insights\n",
    "    if 'publication_year' in df.columns:\n",
    "        year_dist = df['publication_year'].value_counts().head(5)\n",
    "        print(f\"\\n📅 Top publication years:\")\n",
    "        for year, count in year_dist.items():\n",
    "            print(f\"   📈 {year:.0f}: {count:,} papers\")\n",
    "    \n",
    "    if 'journal' in df.columns:\n",
    "        journal_dist = df['journal'].value_counts().head(3)\n",
    "        print(f\"\\n📰 Top journals:\")\n",
    "        for journal, count in journal_dist.items():\n",
    "            print(f\"   📖 {journal}: {count:,} papers\")\n",
    "    \n",
    "    # Text quality metrics\n",
    "    if 'abstract_word_count' in df.columns:\n",
    "        avg_abstract = df['abstract_word_count'].mean()\n",
    "        median_abstract = df['abstract_word_count'].median()\n",
    "        print(f\"\\n📝 Abstract statistics:\")\n",
    "        print(f\"   Average length: {avg_abstract:.0f} words\")\n",
    "        print(f\"   Median length: {median_abstract:.0f} words\")\n",
    "        \n",
    "        # Papers with substantial abstracts (>50 words)\n",
    "        substantial_abstracts = (df['abstract_word_count'] > 50).sum()\n",
    "        print(f\"   Papers with >50 words: {substantial_abstracts:,} ({substantial_abstracts/total_papers*100:.1f}%)\")\n",
    "\n",
    "if df_clean is not None:\n",
    "    assess_data_quality(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9e2b7",
   "metadata": {},
   "source": [
    "# Part 3: Data Analysis and Visualization\n",
    "\n",
    "Now for the main analysis! We'll explore:\n",
    "\n",
    "1. **Publication trends over time** - How has COVID-19 research evolved?\n",
    "2. **Journal analysis** - Which venues publish the most research?  \n",
    "3. **Text analysis** - What topics and terms are most common?\n",
    "4. **Statistical summaries** - Key insights and patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec13f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_publication_trends(df):    \n",
    "    print(\"📈 PUBLICATION TRENDS ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if 'publication_year' not in df.columns:\n",
    "        print(\"❌ No publication year data available\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate yearly statistics\n",
    "    yearly_counts = df['publication_year'].value_counts().sort_index()\n",
    "    \n",
    "    print(f\"📊 Publication timeline:\")\n",
    "    print(f\"   Years covered: {yearly_counts.index.min():.0f} - {yearly_counts.index.max():.0f}\")\n",
    "    print(f\"   Peak year: {yearly_counts.idxmax():.0f} ({yearly_counts.max():,} papers)\")\n",
    "    print(f\"   Total years: {len(yearly_counts)}\")\n",
    "    \n",
    "    # Recent trends (2020 onwards - COVID era)\n",
    "    covid_era = yearly_counts[yearly_counts.index >= 2020] if yearly_counts.index.max() >= 2020 else pd.Series()\n",
    "    if len(covid_era) > 0:\n",
    "        print(f\"\\n🦠 COVID-19 era trends (2020+):\")\n",
    "        for year, count in covid_era.items():\n",
    "            print(f\"   {year:.0f}: {count:,} papers\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Full timeline\n",
    "    axes[0].bar(yearly_counts.index, yearly_counts.values, color='steelblue', alpha=0.7, width=0.8)\n",
    "    axes[0].set_title('COVID-19 Research Publications by Year', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Publication Year')\n",
    "    axes[0].set_ylabel('Number of Papers')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add annotations for peak years\n",
    "    if len(yearly_counts) > 0:\n",
    "        peak_year = yearly_counts.idxmax()\n",
    "        peak_count = yearly_counts.max()\n",
    "        axes[0].annotate(f'Peak: {peak_count:,}', \n",
    "                        xy=(peak_year, peak_count), \n",
    "                        xytext=(peak_year, peak_count * 1.1),\n",
    "                        arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                        ha='center', fontweight='bold')\n",
    "    \n",
    "    # COVID era focus (if applicable)\n",
    "    if len(covid_era) > 1:\n",
    "        axes[1].plot(covid_era.index, covid_era.values, marker='o', linewidth=3, markersize=8, color='darkred')\n",
    "        axes[1].fill_between(covid_era.index, covid_era.values, alpha=0.3, color='darkred')\n",
    "        axes[1].set_title('COVID-19 Era Research Trend', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Year')\n",
    "        axes[1].set_ylabel('Papers Published')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # Show recent 5 years if no COVID era data\n",
    "        recent_years = yearly_counts.tail(min(5, len(yearly_counts)))\n",
    "        axes[1].bar(recent_years.index, recent_years.values, color='darkgreen', alpha=0.7)\n",
    "        axes[1].set_title('Recent Publication Trend', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Year')\n",
    "        axes[1].set_ylabel('Papers Published')\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return yearly_counts\n",
    "\n",
    "if df_clean is not None:\n",
    "    yearly_analysis = analyze_publication_trends(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf738e6c",
   "metadata": {},
   "source": [
    "## Journal Analysis\n",
    "\n",
    "Let's identify the top journals publishing COVID-19 research and understand the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_journals(df, top_n=15):\n",
    "    \"\"\"Analyze journal publication patterns\"\"\"\n",
    "    \n",
    "    print(f\"📰 JOURNAL ANALYSIS (TOP {top_n})\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if 'journal' not in df.columns:\n",
    "        print(\"❌ No journal data available\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate journal statistics\n",
    "    journal_counts = df['journal'].value_counts()\n",
    "    total_papers = len(df)\n",
    "    unique_journals = len(journal_counts)\n",
    "    \n",
    "    print(f\"📊 Journal Overview:\")\n",
    "    print(f\"   Total unique journals: {unique_journals:,}\")\n",
    "    print(f\"   Papers per journal (avg): {total_papers/unique_journals:.1f}\")\n",
    "    \n",
    "    # Top journals\n",
    "    top_journals = journal_counts.head(top_n)\n",
    "    print(f\"\\n🏆 Top {top_n} journals by publication count:\")\n",
    "    \n",
    "    for i, (journal, count) in enumerate(top_journals.items(), 1):\n",
    "        percentage = (count / total_papers) * 100\n",
    "        print(f\"{i:2d}. {journal:<50} {count:>6,} ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # Concentration analysis\n",
    "    top_10_share = (journal_counts.head(10).sum() / total_papers) * 100\n",
    "    top_50_share = (journal_counts.head(50).sum() / total_papers) * 100\n",
    "    \n",
    "    print(f\"\\n📈 Publication concentration:\")\n",
    "    print(f\"   Top 10 journals: {top_10_share:.1f}% of all papers\")\n",
    "    print(f\"   Top 50 journals: {top_50_share:.1f}% of all papers\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 12))\n",
    "    \n",
    "    # Horizontal bar chart of top journals\n",
    "    axes[0].barh(range(len(top_journals)), top_journals.values, color='coral', alpha=0.7)\n",
    "    axes[0].set_yticks(range(len(top_journals)))\n",
    "    axes[0].set_yticklabels([j[:50] + '...' if len(j) > 50 else j for j in top_journals.index])\n",
    "    axes[0].set_xlabel('Number of Papers')\n",
    "    axes[0].set_title(f'Top {top_n} Journals Publishing COVID-19 Research', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    max_count = top_journals.values.max()\n",
    "    for i, (journal, count) in enumerate(top_journals.items()):\n",
    "        axes[0].text(count + max_count * 0.01, i, f'{count:,}', \n",
    "                    va='center', fontweight='bold')\n",
    "    \n",
    "    # Distribution analysis - log scale\n",
    "    journal_sizes = journal_counts.values\n",
    "    axes[1].hist(journal_sizes, bins=50, color='lightblue', alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xlabel('Papers per Journal')\n",
    "    axes[1].set_ylabel('Number of Journals')\n",
    "    axes[1].set_title('Distribution of Papers per Journal', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics to the plot\n",
    "    mean_papers = journal_sizes.mean()\n",
    "    median_papers = np.median(journal_sizes)\n",
    "    axes[1].axvline(mean_papers, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_papers:.1f}')\n",
    "    axes[1].axvline(median_papers, color='orange', linestyle='--', linewidth=2, label=f'Median: {median_papers:.1f}')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return journal_counts\n",
    "\n",
    "if df_clean is not None:\n",
    "    journal_analysis = analyze_journals(df_clean, top_n=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2890c4ca",
   "metadata": {},
   "source": [
    "## Text Analysis\n",
    "\n",
    "Now let's analyze the content of papers through title and abstract text mining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c85cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_content(df, top_words=20):\n",
    "    \"\"\"Analyze text content of papers\"\"\"\n",
    "    \n",
    "    print(f\"🔤 TEXT CONTENT ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if 'title' not in df.columns:\n",
    "        print(\"❌ No title data available\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all titles for analysis\n",
    "    all_titles = ' '.join(df['title'].dropna().astype(str))\n",
    "    \n",
    "    print(f\"📝 Text corpus statistics:\")\n",
    "    print(f\"   Total papers with titles: {df['title'].notna().sum():,}\")\n",
    "    print(f\"   Total characters in titles: {len(all_titles):,}\")\n",
    "    print(f\"   Total words in titles: {len(all_titles.split()):,}\")\n",
    "    \n",
    "    # Word frequency analysis\n",
    "    print(f\"\\n🔍 Performing word frequency analysis...\")\n",
    "    \n",
    "    # Clean and extract words\n",
    "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', all_titles.lower())\n",
    "    \n",
    "    # Define comprehensive stop words for medical/scientific text\n",
    "    stop_words = {\n",
    "        'the', 'and', 'for', 'are', 'with', 'this', 'that', 'from', 'they', 'been', \n",
    "        'have', 'has', 'had', 'was', 'were', 'will', 'would', 'could', 'should',\n",
    "        'can', 'may', 'might', 'must', 'shall', 'not', 'but', 'what', 'when',\n",
    "        'where', 'who', 'how', 'why', 'which', 'than', 'then', 'now', 'here',\n",
    "        'there', 'more', 'most', 'much', 'many', 'some', 'any', 'all', 'both',\n",
    "        'each', 'few', 'other', 'such', 'only', 'own', 'same', 'also', 'just',\n",
    "        'being', 'over', 'through', 'during', 'before', 'after', 'above', 'below',\n",
    "        'between', 'among', 'into', 'within', 'without', 'under', 'again', 'once'\n",
    "    }\n",
    "    \n",
    "    # Filter words\n",
    "    filtered_words = [word for word in words if word not in stop_words and len(word) > 3]\n",
    "    \n",
    "    print(f\"   Words after filtering: {len(filtered_words):,}\")\n",
    "    \n",
    "    # Calculate frequency\n",
    "    word_freq = Counter(filtered_words)\n",
    "    top_words_list = word_freq.most_common(top_words)\n",
    "    \n",
    "    print(f\"\\n🔤 Top {top_words} words in paper titles:\")\n",
    "    for i, (word, count) in enumerate(top_words_list, 1):\n",
    "        percentage = (count / len(filtered_words)) * 100\n",
    "        print(f\"{i:2d}. {word:<15} {count:>6,} occurrences ({percentage:4.1f}%)\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # Word frequency bar chart\n",
    "    words_df = pd.DataFrame(top_words_list, columns=['Word', 'Frequency'])\n",
    "    \n",
    "    axes[0, 0].bar(words_df['Word'], words_df['Frequency'], color='lightgreen', alpha=0.7)\n",
    "    axes[0, 0].set_title(f'Top {top_words} Most Frequent Words in Titles', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Words')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Word cloud\n",
    "    try:\n",
    "        titles_text = ' '.join(filtered_words)\n",
    "        wordcloud = WordCloud(width=800, height=400, \n",
    "                             background_color='white',\n",
    "                             max_words=100,\n",
    "                             collocations=False).generate(titles_text)\n",
    "        \n",
    "        axes[0, 1].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[0, 1].axis('off')\n",
    "        axes[0, 1].set_title('Word Cloud of Paper Titles', fontweight='bold')\n",
    "        \n",
    "    except ImportError:\n",
    "        axes[0, 1].text(0.5, 0.5, 'WordCloud not available\\nInstall: pip install wordcloud', \n",
    "                       ha='center', va='center', transform=axes[0, 1].transAxes, fontsize=12)\n",
    "        axes[0, 1].set_title('Word Cloud (Not Available)')\n",
    "    except Exception as e:\n",
    "        axes[0, 1].text(0.5, 0.5, f'Error creating word cloud:\\n{str(e)}', \n",
    "                       ha='center', va='center', transform=axes[0, 1].transAxes, fontsize=10)\n",
    "    \n",
    "    # Abstract length analysis (if available)\n",
    "    if 'abstract_word_count' in df.columns:\n",
    "        abstract_lengths = df['abstract_word_count']\n",
    "        \n",
    "        axes[1, 0].hist(abstract_lengths[abstract_lengths > 0], bins=50, \n",
    "                       color='lightblue', alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].set_xlabel('Abstract Length (words)')\n",
    "        axes[1, 0].set_ylabel('Number of Papers')\n",
    "        axes[1, 0].set_title('Distribution of Abstract Lengths', fontweight='bold')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics\n",
    "        mean_len = abstract_lengths.mean()\n",
    "        median_len = abstract_lengths.median()\n",
    "        axes[1, 0].axvline(mean_len, color='red', linestyle='--', label=f'Mean: {mean_len:.0f}')\n",
    "        axes[1, 0].axvline(median_len, color='orange', linestyle='--', label=f'Median: {median_len:.0f}')\n",
    "        axes[1, 0].legend()\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'Abstract length data\\nnot available', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes, fontsize=12)\n",
    "        axes[1, 0].set_title('Abstract Length Distribution')\n",
    "    \n",
    "    # Title length analysis\n",
    "    if 'title_length' in df.columns:\n",
    "        title_lengths = df['title_length']\n",
    "        \n",
    "        axes[1, 1].boxplot(title_lengths, patch_artist=True, \n",
    "                          boxprops=dict(facecolor='lightcoral', alpha=0.7))\n",
    "        axes[1, 1].set_ylabel('Title Length (characters)')\n",
    "        axes[1, 1].set_title('Distribution of Title Lengths', fontweight='bold')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics text\n",
    "        mean_title_len = title_lengths.mean()\n",
    "        median_title_len = title_lengths.median()\n",
    "        axes[1, 1].text(0.02, 0.98, f'Mean: {mean_title_len:.0f}\\nMedian: {median_title_len:.0f}', \n",
    "                       transform=axes[1, 1].transAxes, fontsize=10, \n",
    "                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'Title length data\\nnot available', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=12)\n",
    "        axes[1, 1].set_title('Title Length Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return word_freq, top_words_list\n",
    "\n",
    "if df_clean is not None:\n",
    "    text_analysis, top_words_result = analyze_text_content(df_clean, top_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e1938",
   "metadata": {},
   "source": [
    "## Advanced Text Analysis\n",
    "\n",
    "Let's dive deeper into content patterns and research themes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14cb219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_text_analysis(df):\n",
    "    \"\"\"Perform advanced text analysis including COVID-specific terms\"\"\"\n",
    "    \n",
    "    print(\"🔬 ADVANCED TEXT ANALYSIS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    if 'title' not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    # Define COVID-related terms to search for\n",
    "    covid_terms = {\n",
    "        'virus_terms': ['covid', 'coronavirus', 'sars-cov-2', 'pandemic', 'epidemic', 'viral'],\n",
    "        'medical_terms': ['treatment', 'vaccine', 'therapy', 'clinical', 'patient', 'hospital'],\n",
    "        'research_terms': ['study', 'analysis', 'research', 'investigation', 'assessment', 'review'],\n",
    "        'impact_terms': ['impact', 'effect', 'outcome', 'mortality', 'morbidity', 'risk'],\n",
    "        'social_terms': ['social', 'economic', 'psychological', 'mental', 'lockdown', 'isolation']\n",
    "    }\n",
    "    \n",
    "    # Combine all titles for analysis\n",
    "    all_titles_lower = ' '.join(df['title'].dropna().astype(str).str.lower())\n",
    "    \n",
    "    print(\"🎯 COVID-19 research theme analysis:\")\n",
    "    \n",
    "    theme_counts = {}\n",
    "    for theme, terms in covid_terms.items():\n",
    "        theme_name = theme.replace('_', ' ').title()\n",
    "        total_mentions = sum(all_titles_lower.count(term) for term in terms)\n",
    "        papers_with_theme = sum((df['title'].str.contains(term, case=False, na=False)).sum() for term in terms)\n",
    "        \n",
    "        theme_counts[theme_name] = {\n",
    "            'mentions': total_mentions,\n",
    "            'papers': papers_with_theme\n",
    "        }\n",
    "        \n",
    "        print(f\"   {theme_name:<15}: {total_mentions:>4} mentions, {papers_with_theme:>4} papers\")\n",
    "    \n",
    "    # Visualize theme distribution\n",
    "    themes = list(theme_counts.keys())\n",
    "    mentions = [theme_counts[theme]['mentions'] for theme in themes]\n",
    "    papers = [theme_counts[theme]['papers'] for theme in themes]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Theme mentions\n",
    "    bars1 = ax1.bar(themes, mentions, color='skyblue', alpha=0.7)\n",
    "    ax1.set_title('COVID-19 Research Theme Mentions', fontweight='bold')\n",
    "    ax1.set_ylabel('Total Mentions')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars1, mentions):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(mentions)*0.01,\n",
    "                f'{value}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Papers with themes\n",
    "    bars2 = ax2.bar(themes, papers, color='lightcoral', alpha=0.7)\n",
    "    ax2.set_title('Papers Addressing Each Theme', fontweight='bold')\n",
    "    ax2.set_ylabel('Number of Papers')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars2, papers):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(papers)*0.01,\n",
    "                f'{value}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return theme_counts\n",
    "\n",
    "if df_clean is not None:\n",
    "    advanced_analysis = advanced_text_analysis(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3adade",
   "metadata": {},
   "source": [
    "### Create a word cloud from paper titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_title_wordcloud(df):\n",
    "    print(\"\\n☁️ CREATING TITLE WORD CLOUD\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    if 'title' not in df.columns:\n",
    "        print(\"❌ No title data available\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Combine all titles\n",
    "        all_titles = ' '.join(df['title'].dropna().astype(str))\n",
    "        \n",
    "        # Create word cloud\n",
    "        wordcloud = WordCloud(width=1200, height=600, \n",
    "                             background_color='white',\n",
    "                             max_words=100,\n",
    "                             collocations=False).generate(all_titles)\n",
    "        \n",
    "        # Display word cloud\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Word Cloud of Paper Titles', fontsize=18, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"✅ Word cloud created successfully\")\n",
    "        return wordcloud\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"❌ WordCloud library not available. Install with: pip install wordcloud\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eb309d",
   "metadata": {},
   "source": [
    "### Analyze distribution of papers by source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c0d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sources(df):\n",
    "    print(\"\\n📚 SOURCE DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Try different possible source columns\n",
    "    source_columns = ['source_x', 'source', 'database', 'origin']\n",
    "    source_col = None\n",
    "    \n",
    "    for col in source_columns:\n",
    "        if col in df.columns:\n",
    "            source_col = col\n",
    "            break\n",
    "    \n",
    "    if source_col is None:\n",
    "        print(\"❌ No source column found\")\n",
    "        return None\n",
    "    \n",
    "    # Count papers by source\n",
    "    source_counts = df[source_col].value_counts()\n",
    "    \n",
    "    print(f\"📊 Papers by source:\")\n",
    "    for source, count in source_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   • {source}: {count:,} papers ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Plot source distribution\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(source_counts)))\n",
    "    plt.pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%',\n",
    "            colors=colors, startangle=90)\n",
    "    plt.title('Distribution of Papers by Source', fontsize=16, fontweight='bold')\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return source_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e990fedb",
   "metadata": {},
   "source": [
    "###  Generate comprehensive summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_stats(df):\n",
    "    print(\"\\n📈 SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    total_papers = len(df)\n",
    "    print(f\"📄 Total papers: {total_papers:,}\")\n",
    "    \n",
    "    if 'abstract_word_count' in df.columns:\n",
    "        avg_abstract_length = df['abstract_word_count'].mean()\n",
    "        print(f\"📝 Average abstract length: {avg_abstract_length:.0f} words\")\n",
    "    \n",
    "    if 'publication_year' in df.columns:\n",
    "        year_range = f\"{df['publication_year'].min():.0f} - {df['publication_year'].max():.0f}\"\n",
    "        print(f\"📅 Publication year range: {year_range}\")\n",
    "    \n",
    "    if 'journal' in df.columns:\n",
    "        unique_journals = df['journal'].nunique()\n",
    "        print(f\"📰 Unique journals: {unique_journals:,}\")\n",
    "    \n",
    "    if 'authors' in df.columns:\n",
    "        papers_with_authors = df['authors'].notna().sum()\n",
    "        print(f\"👥 Papers with author info: {papers_with_authors:,} ({papers_with_authors/total_papers*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac78d905-62aa-4404-afc6-d842a467c4b4",
   "metadata": {},
   "source": [
    "## Main analysis workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c68bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"🦠 CORD-19 METADATA ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Part 1: Data Loading and Basic Exploration\n",
    "    print(\"\\n\" + \"=\"*20 + \" PART 1: DATA LOADING \" + \"=\"*20)\n",
    "    df = load_cord19_data()\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    dtypes, missing_counts = explore_basic_info(df)\n",
    "    missing_analysis = analyze_missing_data(df)\n",
    "    \n",
    "    # Part 2: Data Cleaning and Preparation\n",
    "    print(\"\\n\" + \"=\"*20 + \" PART 2: DATA CLEANING \" + \"=\"*20)\n",
    "    df_clean = clean_data(df)\n",
    "    \n",
    "    # Part 3: Data Analysis and Visualization\n",
    "    print(\"\\n\" + \"=\"*20 + \" PART 3: ANALYSIS & VISUALIZATION \" + \"=\"*20)\n",
    "    yearly_analysis = analyze_publications_by_year(df_clean)\n",
    "    journal_analysis = analyze_top_journals(df_clean)\n",
    "    word_analysis = analyze_title_words(df_clean)\n",
    "    wordcloud = create_title_wordcloud(df_clean)\n",
    "    source_analysis = analyze_sources(df_clean)\n",
    "    \n",
    "    # Generate summary\n",
    "    generate_summary_stats(df_clean)\n",
    "    \n",
    "    print(\"\\n✅ Analysis completed!\")\n",
    "    return df_clean\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_final = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
